<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/compare" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo_teleai.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <style>
    .gradient-text {
      background: linear-gradient(90deg, rgba(131,58,180,1) 0%, rgba(253,29,29,1) 50%, rgba(252,176,69,1) 100%);
      -webkit-background-clip: text;
      color: transparent;
      background-clip: text;
    }
    .title span {
        background: -webkit-linear-gradient(45deg, #7ed56f, #28b485);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .large-text {
      font-size: 56px;
    }
    .subscript-emoji {
            font-size: 12px;
            vertical-align: sub;
        }
    code {
      white-space: pre-wrap;
    }

    body {
      font-family: 'Jost';
    }

    b {
      font-weight: black;
    }

    strong {
      font-weight: bold;
    }

    section>h5 {
      padding-bottom: 30px;
    }

    .col-12>h3 {
      padding-top: 30px;
    }

    .results-section {
      padding-bottom: 30px;
    }

    .carousel-control-prev,
    .carousel-control-next {
      background-color: rgba(0, 0, 0, 0.5);
      border-radius: 50%;
      width: 50px;
      height: 50px;
      display: flex;
      align-items: center;
      justify-content: center;
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
    }

    .carousel-control-prev {
      left: 25px;
    }

    .carousel-control-next {
      right: 25px;
    }

    .carousel-indicators {
      display: flex;
      align-items: center;
      justify-content: center;
      position: absolute;
      bottom: 10%;
      left: 90%;

      /* Adjust vertical position */
      text-align: center;
      /* Center the content horizontally */
      margin: 0;
      padding: 0;
      list-style: none;
    }

    .carousel-indicators li {
      background-color: transparent;
      /* Hide the default Bootstrap indicators */
      border: none;
      /* Remove border */
      margin: 0 25px;
      display: inline-block;
    }

    .indicator-dot {
      display: inline-block;
      width: 15px;
      height: 15px;
      text-indent: -999px;
      border: none;
      border-radius: 10px;
      background-color: rgba(0, 0, 0, 0.5);
    }

    .row.sm-gutters {
      margin-right: 5px;
      margin-left: 5px;
      margin-top: 5px;
      margin-bottom: 5px;
    }
    .row.sm-gutters-2 {
        justify-content: center;
        align-items:center;
    }

    .row.sm-gutters>.col-md-4 {
      padding-right: 5px;
      padding-left: 5px;
      padding-top: 5px;
      padding-bottom: 5px;
    }

    .row.sm-gutters>.col-md-6 {
      padding-right: 5px;
      padding-left: 5px;
      padding-top: 5px;
      padding-bottom: 5px;
    }
  .row.sm-gutters>.col-md-6-2 {
      justify-content: center;
      align-items:center;
  }
  .row.sm-gutters>.col-md-6-3 {
      padding-right: 5px;
      padding-left: 5px;
      padding-top: 5px;
      padding-bottom: 5px;
  }

    .row.sm-gutters>.col-md-12 {
      padding-top: 5px;
      padding-bottom: 5px;
      padding-right: 5px;
      padding-left: 5px;
    }

    .renbody-zoom-container {
      width: 80%;
      /* Or whatever size you want */
      aspect-ratio: 0.75;
      /* Or whatever size you want */
      overflow: hidden;
      /* Hide the parts of the video outside the container */
      object-fit: cover;
      /* Make sure the video covers the container without stretching */
      object-position: center;
      /* Center the video */
      border-radius: 5px;
    }

    .mobile-zoom-container {
      width: 100%;
      /* Or whatever size you want */
      aspect-ratio: 0.75;
      /* Or whatever size you want */
      overflow: hidden;
      /* Hide the parts of the video outside the container */
      object-fit: cover;
      /* Make sure the video covers the container without stretching */
      object-position: center;
      /* Center the video */
      border-radius: 5px;
      margin: 5px
    }

    .video-zoom-container {
      overflow: hidden;
      /* Hide the parts of the video outside the container */
      object-fit: cover;
      /* Make sure the video covers the container without stretching */
      object-position: center;
      border-radius: 5px;
      margin: 5px;
    }

    .zoom-element {
      transition: transform 0.2s ease-in-out;
    }

    .zoom-element:hover {
      transform: scale(1.65);
    }

    .nhr-zoom {
      transform: scale(1.45);
    }

    .renbody-zoom {
      transform: scale(1.65) translateY(2%);
      border-radius: 5px;
    }

    .mobile-zoom {
      transform: scale(1.65);
      border-radius: 5px;
    }

    .zjumocap-zoom {
      transform: scale(1.3);
    }

    video {
      border-radius: 5px;
      padding: 0px;
      margin: 0px;
    }

    .comparison-canvas {
      padding: 0px;
      margin: 0px;
      border-radius: 5px;
    }

    .a16x9 {
      aspect-ratio: 16/9;
    }

    .a4x3 {
      aspect-ratio: 1352/1014;
    }

    .a1x1 {
      aspect-ratio: 1;
    }

    .a16x9demo {
      aspect-ratio: 1920/1152;
    }

    .a4x3demo {
      aspect-ratio: 1352/1086;
    }

    .a1x1demo {
      aspect-ratio: 1600/1672;
    }
  
  img {
        display: block;
        margin: 0 auto;
      }
  
  </style>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 style="font-weight: bold;">
            <span class="gradient-text large-text">Beyond Uncertainty</span>
            <span class="large-text">:Evidential Deep Learning for Robust Video Temporal Grounding</span>
            </h1>
            <h1 class="title is-1 publication-title"></h1>
            <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
                <a href="https://kaijing.space" target="_blank">Kaijing Ma</a><sup>ðŸŽ“ðŸ¥°*</sup>,
            </span>
            <span class="author-block">
                <a href="https://github.com/JethroJames" target="_blank">Haojian Huang</a><sup>ðŸŽ“ðŸ˜½*</sup>,
            </span>
            <span class="author-block">
              <a href="https://dunknsabsw.github.io" target="_blank">Jin Chen</a><sup>ðŸŽ“ðŸ¥°*</sup>,
            </span><br>
            <span class="author-block">
                <a href="https://haroldchen19.github.io" target="_blank">Haodong Chen</a><sup>ðŸ¥³ðŸŽ“</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/pengliang-ji-a73300206/" target="_blank">Pengliang Ji</a><sup>ðŸ˜‰</sup>,
            </span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=JHN-jlAAAAAJ&hl=zh-CN&oi=ao" target="_blank">Xianghao Zang</a><sup>ðŸŽ“</sup>,
            </span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=kjeKM7gAAAAJ&hl=zh-CN" target="_blank">Han Fang</a><sup>ðŸŽ“</sup>,
            </span>
            <span class="author-block">
                Chao Ban<sup>ðŸŽ“</sup>,
            </span><br>
            <span class="author-block">
                Hao Sun<sup>ðŸŽ“</sup>,
            </span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=8P7q1wQAAAAJ&hl=zh-CN&oi=ao" target="_blank">Mulin Chen</a><sup>ðŸŽ“</sup>,
            </span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN&oi=ao" target="_blank">Xuelong Li</a><sup>ðŸŽ“</sup>
            </span>
            </div>

                  <div class="is-size-5 publication-authors"><br>
                    <span class="author-block">ðŸŽ“ TeleAI<br>ðŸ¥° Xi'an Jiaotong University<br>ðŸ˜½ The University of Hong Kong<br>ðŸ¥³ Northwestern Polytechnical University<br>ðŸ˜‰ Carnegie Mellon University</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="eql-cntrb"><small><br>ðŸŽŠ We have released the arXiv preprint, supplementary materials, and part of the code.</small></span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2408.16272" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/KaijingOfficial/sram_vtg" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">

<div class="item">
      <img src="static/images/compare.jpg" width="500" height="auto"/>
      <h2 class="subtitle has-text-centered">
        
        <br>Our proposed SRAM quantify uncertainty, thus allowing  <br> the model to say "I do not know" in scenarios beyond its handling capacity. 
      </h2>
</div>
<br>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing Video Temporal Grounding (VTG) models excel in accuracy but often overlook open-world challenges posed by open-vocabulary queries and untrimmed videos. This leads to unreliable predictions for noisy, corrupted, and out-of-distribution data. Adapting VTG models to dynamically estimate uncertainties based on user input can address this issue. To this end, we introduce SRAM, a robust network module that benefits from a two-stage cross-modal alignment task. More importantly, it integrates Deep Evidential Regression (DER) to explicitly and thoroughly quantify uncertainty during training, thus allowing the model to say "I do not know" in scenarios beyond its handling capacity. However, the direct application of traditional DER theory and its regularizer reveals structural flaws, leading to unintended constraints in VTG tasks. In response, we develop a simple yet effective Geom-regularizer that enhances the uncertainty learning framework from the ground up. To the best of our knowledge, this marks the first successful attempt of DER in VTG. Our extensive quantitative and qualitative results affirm the effectiveness, robustness, and interpretability of our modules and the uncertainty learning paradigm in VTG tasks. The code will be made available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="item">
          <img src="static/images/model.png" width="800" height="auto"/>
        </div>
        <div class="content has-text-justified">
          <p>
            <br>
            Overall architecture of SRAM. Firstly, an untrimmed video and masked query are encoded with a frozen encoder, then SRAM reconstructs the masked query tokens. In the second stage, SRAM performs temporal grounding on the video using the complete user's query. SRAM includes RFF blocks, an evidential head, a VTG head, and a Masked Language Model (MLM) head. The MLM head enclosed by the dashed box is trained only during the first stage.
          </p>
          <h3 class="title is-4">Learning Uncertainty with DER</h3>
          <p>Temporal continuity in videos often causes adjacent frames to share similar semantics, complicating precise boundary delineation and introducing subjective biases in annotations. To mitigate this, we model semantic boundary moments using Gaussian distributions. Specifically, the start and end moments of a video-query pair \( (V, Q) \) are each governed by distinct Gaussian distributions. Observations of the same type (either all starts or all ends) are assumed to be i.i.d.. Without loss of generality, we formulate as follows:</p>

            \[
            b \sim\mathcal{N}(\mu,\sigma^2),
            \]

            where \( b \in \mathbb{R}^{1 \times \mathcal{H}}\) represents the start or end of moments observed \(\mathcal{H}\) times. The corresponding expectation \(\mu\) and variance \(\sigma^2\) of the Gaussian distribution subject to \( NIG \) prior:
            
            \[
            \begin{align}
            p(\mu,\sigma^2\mid\underbrace{\gamma,\upsilon,\alpha,\beta}_{\boldsymbol{\varphi }})
            &=
            \mathcal{N}(\mu|\gamma,\sigma^2 \upsilon^{-1})
            \Gamma^{-1}(\sigma^2|\alpha,\beta) \\
            &=\frac{\beta^\alpha\sqrt{\upsilon}}{\Gamma(\alpha)\sqrt{2\pi\sigma^2}}\left(\frac{1}{\sigma^2}\right)^{\alpha+1}
            \exp\left\{-\frac{2\beta+\upsilon(\gamma-\mu)^2}{2\sigma^2}\right\}.
            \end{align}
            \]

            where \(\boldsymbol{\varphi}=(\gamma, \upsilon, \alpha, \beta)\) are the prior \( NIG\) distribution parameters derived from the video content and user queries, serve as conditionals for the Gaussian estimates of \( b_i \), with \(\gamma \in \mathbb{R}, \upsilon > 0, \alpha > 1, \beta > 0\). The gamma function is denoted by \( \Gamma(\cdot)\). We use a linear evidential predictor to estimate \(\boldsymbol{\varphi }\), training it to maximize the likelihood. The maximum likelihood estimation for \(b_i\) is given by:
            
            \[
            p(b_i \mid \boldsymbol{\varphi }) = \int_{\sigma^2=0}^\infty \int_{\mu=-\infty}^\infty p(b_i \mid \mu, \sigma^2) p(\mu, \sigma^2 \mid \boldsymbol{\varphi }) d\mu d\sigma^2 = \mathrm{St}(b_i; \gamma, \frac{\beta(1 + \upsilon)}{\upsilon \alpha}, 2\alpha).
            \]

            Since the likelihood function has a form of Student-t distribution \((\mathrm{St})\), we minimize the negative logarithmic likelihood (NLL) as follows.

            \[
            \mathcal{L}^{\mathrm{NLL}}_{i}=-\log p(b_i|\boldsymbol{\varphi })= -\log\left(\mathrm{St}\left(b_i;\gamma,\frac{\beta(1+\upsilon)}{\upsilon\alpha},2\alpha\right)\right).
            \]
          
          <h3 class="title is-4">A better regularizer: Geom-regularization </h3>
          
          Models optimized only on observed samples with the NLL loss tend to overfit and exhibit overconfidence:
          <br>
          <br>
          <div class="item">
            <img src="static/images/scatter.jpg" width="800" height="auto"/>
          </div>
          <br>

          Actually, the original DER  propose a heuristic regularization, which aims to mitigate overconfidence by suppressing evidence, particularly for samples with high error:
          \[
          \mathcal{L}^\mathrm{R}_i(\boldsymbol{\vartheta})=\Delta\cdot\Phi.
          \] 
          where \( \Delta = |b_i-\gamma| \) represents the error, \(\Phi = 2\upsilon+\alpha\) denotes the evidence, and \(\boldsymbol{\vartheta}\) are the model parameters, with \(b_i\) as the ground truth.
          
          <br>
          <br>
          Unfortunately, the vanilla regularizer tends to excessively suppress the evidence, as shown in the visualized gradient field.

          
          <br>
          <div class="item">
            <img src="static/images/gardient.jpg" width="800" height="auto"/>
          </div>
          <br>
          To overcome these limitations, we introduce Geom-regularization, promoting the principle that "accurate predictions should have high evidence, while inaccurate ones should have low evidence":
          
          \[
          \mathcal{L}^i(\boldsymbol{w})=\|\overline{\Phi}+\overline{\Delta}-1\|^2_2.
          \] 
        
          <br>
          <br>
          Please refer to our paper for more analysis. 
          
        </div>
          
          
      </div>
    </div>
  </div>
</section>
<!-- Image carousel -->

<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{ma2024beyond,
  title={Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding},
  author={Ma, Kaijing and Huang, Haojian and Chen, Jin and Chen, Haodong and Ji, Pengliang and Zang, Xianghao and Fang, Han and Ban, Chao and Sun, Hao and Chen, Mulin and others},
  journal={arXiv preprint arXiv:2408.16272},
  year={2024}
}   </code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
